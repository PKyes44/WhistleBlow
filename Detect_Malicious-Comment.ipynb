{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMWrlLThqP/4zzH8LIkpxgm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install transformers[torch]\n","!git clone https://github.com/ZIZUN/korean-malicious-comments-dataset.git\n","!pip install openai\n","!pip install openpyxl\n","!pip install flask-ngrok"],"metadata":{"id":"ny2UIogAuTBn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688454640936,"user_tz":-540,"elapsed":23207,"user":{"displayName":"양은석_세종장영실고","userId":"04431019167965294214"}},"outputId":"fc392d9f-82f5-4cc1-93b4-32072d4a7e53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n","Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Requirement already satisfied: accelerate>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n","fatal: destination path 'korean-malicious-comments-dataset' already exists and is not an empty directory.\n","Collecting openai\n","  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n","Installing collected packages: openai\n","Successfully installed openai-0.27.8\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.0.10)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n","Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.27.1)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.3.6)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.3)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dFQLTzXhuEj1"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(\"device:\", device)\n","\n","# 데이터셋 생성\n","class CurseDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(\n","            self.labels[idx]\n","            # RuntimeError: expected scalar type Long but found Int\n","            , dtype=torch.long\n","        )\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# 학습과정에서 사용할 평가지표를 위한 함수 설정\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,            # 정확도\n","        'f1': f1,                   # F1 스코어\n","        'precision': precision,     # 정밀도\n","        'recall': recall            # 재현율\n","    }\n","\n","\n","# 댓글과 레이블은 탭(\"\\t\")로 구분되어있으므로\n","# sep 파라미터로 \"\\t\" 사용\n","df = pd.read_csv(\"/content/korean-malicious-comments-dataset/Dataset.csv\", sep=\"\\t\")\n","df.head()\n","\n","null_idx = df[df.lable.isnull()].index\n","df.loc[null_idx, \"content\"]\n","\n","# ---- 전처리 ----\n","# lable은 content의 가장 끝 문자열로 설정\n","df.loc[null_idx, \"lable\"] = df.loc[null_idx, \"content\"].apply(lambda x: x[-1])\n","\n","#content는 \"\\t\" 앞부분까지의 문자열로 설정\n","df.loc[null_idx, \"content\"] = df.loc[null_idx, \"content\"].apply(lambda x: x[:-2])\n","\n","# 학습을 위해 lable의 데이터타입을 float -> int\n","df = df.astype({\"lable\":\"int\"})\n","\n","df.info()\n","# --------\n","\n","# Train set / Test set 구분\n","train_data = df.sample(frac=0.8, random_state=42)\n","test_data = df.drop(train_data.index)\n","\n","# 데이터셋 개수 확인\n","print('중복 제거 전 학습 데이터셋 : {}'.format(len(train_data)))\n","print('중복 제거 전 검증 데이터셋 : {}'.format(len(test_data)))\n","\n","# 중복 데이터 제거\n","train_data.drop_duplicates(subset=[\"content\"], inplace=True)\n","test_data.drop_duplicates(subset=[\"content\"], inplace=True)\n","\n","# 데이터셋 개수 확인\n","print('중복 제거 후 학습 데이터셋 : {}'.format(len(train_data)))\n","print('중복 제거 후 검증 데이터셋 : {}'.format(len(test_data)))\n","\n","# 토크나이저 가져오기\n","MODEL_NAME = \"beomi/KcELECTRA-base\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","# 토크나이징\n","tokenized_train_sentences = tokenizer(\n","    list(train_data[\"content\"]),\n","    return_tensors=\"pt\",\n","    max_length=128,\n","    padding=True,\n","    truncation=True,\n","    add_special_tokens=True,\n",")\n","\n","tokenized_test_sentences = tokenizer(\n","    list(test_data[\"content\"]),\n","    return_tensors=\"pt\",\n","    max_length=128,\n","    padding=True,\n","    truncation=True,\n","    add_special_tokens=True,\n",")\n","\n","train_label = train_data[\"lable\"].values\n","test_label = test_data[\"lable\"].values\n","\n","train_dataset = CurseDataset(tokenized_train_sentences, train_label)\n","test_dataset = CurseDataset(tokenized_test_sentences, test_label)\n","\n","# 모델 불러오기\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n","model.to(device)\n","\n","# 학습 파라미터 설정\n","training_args = TrainingArguments(\n","    output_dir='./',                # 학습결과 저장경로\n","    num_train_epochs=10,            # 학습 epoch 설정\n","    per_device_train_batch_size=8,  # train batch_size 설정\n","    per_device_eval_batch_size=64,  # test batch_size 설정\n","    logging_dir='./logs',           # 학습 log 저장경로\n","    logging_steps=500,              # 학습 log 기록 단위\n","    save_total_limit=2,             # 학습결과 저장 최대개수\n",")\n","\n","\n","# 모델의 학습 컨트롤\n","trainer = Trainer(\n","    model=model,                        # 학습하고자 하는 모델\n","    args=training_args,                 # 위에서 정의한 Training Arguments\n","    train_dataset=train_dataset,        # 학습 데이터셋\n","    eval_dataset=test_dataset,          # 평가 데이터셋\n","    compute_metrics=compute_metrics,    # 평가지표\n",")\n","\n","# 학습\n","trainer.train()\n","\n","def Re_train(self, directory):\n","  # 댓글과 레이블은 탭(\"\\t\")로 구분되어있으므로\n","  # sep 파라미터로 \"\\t\" 사용\n","  df = pd.read_csv(\"/content/korean-malicious-comments-dataset/Dataset.csv\", sep=\"\\t\")\n","  df.head()\n","\n","  null_idx = df[df.lable.isnull()].index\n","  df.loc[null_idx, \"content\"]\n","\n","  # ---- 전처리 ----\n","  # lable은 content의 가장 끝 문자열로 설정\n","  df.loc[null_idx, \"lable\"] = df.loc[null_idx, \"content\"].apply(lambda x: x[-1])\n","\n","  #content는 \"\\t\" 앞부분까지의 문자열로 설정\n","  df.loc[null_idx, \"content\"] = df.loc[null_idx, \"content\"].apply(lambda x: x[:-2])\n","\n","  # 학습을 위해 lable의 데이터타입을 float -> int\n","  df = df.astype({\"lable\":\"int\"})\n","\n","  df.info()\n","  # --------\n","\n","  # Train set / Test set 구분\n","  train_data = df.sample(frac=0.8, random_state=42)\n","  test_data = df.drop(train_data.index)\n","\n","  # 데이터셋 개수 확인\n","  print('중복 제거 전 학습 데이터셋 : {}'.format(len(train_data)))\n","  print('중복 제거 전 검증 데이터셋 : {}'.format(len(test_data)))\n","\n","  # 중복 데이터 제거\n","  train_data.drop_duplicates(subset=[\"content\"], inplace=True)\n","  test_data.drop_duplicates(subset=[\"content\"], inplace=True)\n","\n","  # 데이터셋 개수 확인\n","  print('중복 제거 후 학습 데이터셋 : {}'.format(len(train_data)))\n","  print('중복 제거 후 검증 데이터셋 : {}'.format(len(test_data)))\n","\n","  # 토크나이저 가져오기\n","  MODEL_NAME = \"beomi/KcELECTRA-base\"\n","  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","  # 토크나이징\n","  tokenized_train_sentences = tokenizer(\n","      list(train_data[\"content\"]),\n","      return_tensors=\"pt\",\n","      max_length=128,\n","      padding=True,\n","      truncation=True,\n","      add_special_tokens=True,\n","  )\n","\n","  tokenized_test_sentences = tokenizer(\n","      list(test_data[\"content\"]),\n","      return_tensors=\"pt\",\n","      max_length=128,\n","      padding=True,\n","      truncation=True,\n","      add_special_tokens=True,\n","  )\n","\n","  train_label = train_data[\"lable\"].values\n","  test_label = test_data[\"lable\"].values\n","\n","  train_dataset = CurseDataset(tokenized_train_sentences, train_label)\n","  test_dataset = CurseDataset(tokenized_test_sentences, test_label)\n","\n","  # 모델 불러오기\n","  model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n","  model.to(device)\n","\n","  # 학습 파라미터 설정\n","  training_args = TrainingArguments(\n","      output_dir='./',                # 학습결과 저장경로\n","      num_train_epochs=10,            # 학습 epoch 설정\n","      per_device_train_batch_size=8,  # train batch_size 설정\n","      per_device_eval_batch_size=64,  # test batch_size 설정\n","      logging_dir='./logs',           # 학습 log 저장경로\n","      logging_steps=500,              # 학습 log 기록 단위\n","      save_total_limit=2,             # 학습결과 저장 최대개수\n","  )\n","\n","\n","  # 모델의 학습 컨트롤\n","  trainer = Trainer(\n","      model=model,                        # 학습하고자 하는 모델\n","      args=training_args,                 # 위에서 정의한 Training Arguments\n","      train_dataset=train_dataset,        # 학습 데이터셋\n","      eval_dataset=test_dataset,          # 평가 데이터셋\n","      compute_metrics=compute_metrics,    # 평가지표\n","  )\n","\n","  # 학습\n","  trainer.train()\n","\n","  return True\n","\n","\n"]},{"cell_type":"code","source":["trainer.evaluate(eval_dataset=test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"onKioYiF5aPi","executionInfo":{"status":"ok","timestamp":1688439177278,"user_tz":-540,"elapsed":13305,"user":{"displayName":"양은석_세종장영실고","userId":"04431019167965294214"}},"outputId":"b780b184-63bd-4f29-9a08-d7994a7c4687"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-3fb2416337a2>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32/32 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.8223304152488708,\n"," 'eval_accuracy': 0.902,\n"," 'eval_f1': 0.9009100101112235,\n"," 'eval_precision': 0.8981854838709677,\n"," 'eval_recall': 0.9036511156186613,\n"," 'eval_runtime': 13.1488,\n"," 'eval_samples_per_second': 152.105,\n"," 'eval_steps_per_second': 2.434,\n"," 'epoch': 10.0}"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfrhHyDG2SRC","executionInfo":{"status":"ok","timestamp":1688448713512,"user_tz":-540,"elapsed":11251,"user":{"displayName":"양은석_세종장영실고","userId":"04431019167965294214"}},"outputId":"1cb43939-d2ae-443b-d71c-fae7fb1a8c75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.0.10)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n","Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.27.1)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.3.6)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.3)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","source":["from pandas.io.parsers.readers import csv\n","import time\n","import openai\n","from flask import Flask\n","from flask import request\n","import openpyxl\n","import os.path\n","from os import path\n","from flask_ngrok import run_with_ngrok\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)\n","\n","openai.api_key = \"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\"\n","\n","\n","def Training(comment):\n","    new_filename = 'content/data.xlsx'\n","    if not path.exists(new_filename):\n","      # 엑셀 만들기\n","      wb = openpyxl.Workbook()\n","    else :\n","      # 엑셀 열기\n","      wb = openpyxl.load_workbook(new_filename)\n","\n","    # 현 시트 선택\n","    ws = wb.active\n","\n","    # 엑셀 저장\n","    ws.append([comment])\n","    wb.save(new_filename)\n","    # csv에서 일정 개수 넘어서면 학습\n","    if ws['A5'].value != None:\n","      # 쌓인 데이터 학습\n","      Re_train(new_filename)\n","      # csv 삭제 후 재생성\n","      os.remove(new_filename)\n","      wb = openpyxl.Workbook()\n","      wb.save(new_filename)\n","      return True\n","    return True\n","\n","def Is_bad_comment(comment):\n","    # 학습된 데이터를 토대로 악플인지 먼저 확인하고,\n","    is_bad_comment = Sentence_predict(comment)\n","    # 악플인 경우 => 악플로 return\n","    if is_bad_comment:\n","      return True\n","    # 악플이 아닌 경우(선플인 경우)\n","    # chatGPT를 통해 다시 한 번 확인한 후, 학습 여부를 선택\n","    query = '다음 문장은 악플이야? \"' + comment + '\"'\n","    completion = openai.ChatCompletion.create(\n","      model=\"gpt-3.5-turbo\",\n","      messages=[\n","        {\"role\": \"user\", \"content\": query}\n","      ]\n","    )\n","    answer = completion.choices[0].message.content\n","    print(query, '-', answer)\n","\n","    if answer.find(\"네, \") >= 0 or answer.find(\"예, \") >= 0 or answer.find(\"악플입니다\") >= 0 or answer.find(\"악플이다\") >= 0 or answer.find(\"악플로 볼 수 있\") >= 0:\n","        return True\n","\n","    return False\n","\n","@app.route('/analyze', methods=['GET', 'POST'])\n","def Analyze():\n","    comment = request.form.get('comment')\n","    sender = request.form.get('sender')\n","    regDate = request.form.get('regDate')\n","    if Is_bad_comment(comment):\n","        Training(comment)\n","        return jsonify(\n","          \"comment\"=comment,\n","          \"sender\"=sender,\n","          \"regDate\"=regDate,\n","          \"result\"=\"bad_comment\"\n","        )\n","    return jsonify(\n","        \"comment\"=comment,\n","        \"sender\"=sender,\n","        \"regDate\"=regDate,\n","        \"result\"=\"normal_comment\"\n","    )\n","\n","@app.route('/')\n","def hello_world():\n","    return 'Hello World!'\n","\n","if __name__ == '__main__':\n","    app.run()\n","\n","\n"],"metadata":{"id":"SNVPo_kr2RAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Sentence_predict(sent):\n","  # 평가모드로 변경\n","  model.eval()\n","\n","  # 입력된 문장 토크나이징\n","  tokenized_sent = tokenizer(\n","      sent,\n","      return_tensors = \"pt\",\n","      truncation=True,\n","      add_special_tokens=True,\n","      max_length=128\n","  )\n","\n","  # 모델이 위치한 GPU로 이동\n","  tokenized_sent.to(device)\n","\n","  # 예측\n","  with torch.no_grad():\n","    outputs = model(\n","        input_ids=tokenized_sent[\"input_ids\"],\n","        attention_mask=tokenized_sent[\"attention_mask\"],\n","        token_type_ids=tokenized_sent[\"token_type_ids\"]\n","    )\n","\n","  # 결과\n","  logits = outputs[0]\n","  logits = logits.detach().cpu()\n","  result = logits.argmax(-1)\n","  if result == 0:\n","    result = True\n","  elif result == 1:\n","    result = False\n","  return result\n","\n","while True:\n","  sentence = input(\"댓글을 입력해주세요 : \")\n","  if sentence == \"quit\":\n","    break\n","  print(sentence_predict(sentence))\n","  print(\"\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_eMX-dr5vrB","executionInfo":{"status":"ok","timestamp":1687434588984,"user_tz":-540,"elapsed":8265,"user":{"displayName":"양은석_세종장영실고","userId":"04431019167965294214"}},"outputId":"f3a5c570-32bc-4a8c-b476-27bbee13159e"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["댓글을 입력해주세요 : qwe\n",">> 착한댓글\n","\n","\n","댓글을 입력해주세요 : quit\n"]}]}]}